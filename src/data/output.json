{"repo_id":"100-times-faster-nlp","file_structure":"{\".\": {\"_files\": [\".gitignore\", \"100-times-faster-nlp-in-python.html\", \"100-times-faster-nlp-in-python.ipynb\", \"README.md\"]}, \".git\": {\"_files\": [\"config\", \"description\", \"HEAD\", \"index\", \"packed-refs\"], \"hooks\": {\"_files\": [\"applypatch-msg.sample\", \"commit-msg.sample\", \"fsmonitor-watchman.sample\", \"post-update.sample\", \"pre-applypatch.sample\", \"pre-commit.sample\", \"pre-merge-commit.sample\", \"pre-push.sample\", \"pre-rebase.sample\", \"pre-receive.sample\", \"prepare-commit-msg.sample\", \"push-to-checkout.sample\", \"sendemail-validate.sample\", \"update.sample\"]}, \"info\": {\"_files\": [\"exclude\"]}, \"logs\": {\"_files\": [\"HEAD\"], \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"master\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}}}, \"objects\": {\"_files\": [], \"info\": {\"_files\": []}, \"pack\": {\"_files\": [\"pack-bb303831e6f5d0939c0cb53da5923af51be6a70f.idx\", \"pack-bb303831e6f5d0939c0cb53da5923af51be6a70f.pack\", \"pack-bb303831e6f5d0939c0cb53da5923af51be6a70f.rev\"]}}, \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"master\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}, \"tags\": {\"_files\": []}}}}","readme_content":"# \ud83d\ude80 100 Times Faster Natural Language Processing in Python\n\nThis repository contains an iPython notebook accompanying the post [\ud83d\ude80100 Times Faster Natural Language Processing in Python](https:\/\/medium.com\/huggingface\/100-times-faster-natural-language-processing-in-python-ee32033bdced).\n\nThe notebook contains all the examples of the post running in a iPython session.\n\nOnline, the notebook can be better visualized [on nbviewer](https:\/\/nbviewer.jupyter.org\/github\/huggingface\/100-times-faster-nlp\/blob\/master\/100-times-faster-nlp-in-python.ipynb) (Github's ipynb visualizer doesn't render well Cython interactive annotations).\n"}
{"repo_id":"accelerate","file_structure":"{\".\": {\"_files\": [\".gitignore\", \".pre-commit-config.yaml\", \"CODE_OF_CONDUCT.md\", \"CONTRIBUTING.md\", \"LICENSE\", \"Makefile\", \"pyproject.toml\", \"README.md\", \"setup.py\"]}, \".devcontainer\": {\"_files\": [\"devcontainer.json\"]}, \".git\": {\"_files\": [\"config\", \"description\", \"HEAD\", \"index\", \"packed-refs\"], \"hooks\": {\"_files\": [\"applypatch-msg.sample\", \"commit-msg.sample\", \"fsmonitor-watchman.sample\", \"post-update.sample\", \"pre-applypatch.sample\", \"pre-commit.sample\", \"pre-merge-commit.sample\", \"pre-push.sample\", \"pre-rebase.sample\", \"pre-receive.sample\", \"prepare-commit-msg.sample\", \"push-to-checkout.sample\", \"sendemail-validate.sample\", \"update.sample\"]}, \"info\": {\"_files\": [\"exclude\"]}, \"logs\": {\"_files\": [\"HEAD\"], \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"main\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}}}, \"objects\": {\"_files\": [], \"info\": {\"_files\": []}, \"pack\": {\"_files\": [\"pack-4188da51619783d6e0df453ce8c8ca3154cd88a5.idx\", \"pack-4188da51619783d6e0df453ce8c8ca3154cd88a5.pack\", \"pack-4188da51619783d6e0df453ce8c8ca3154cd88a5.rev\"]}}, \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"main\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}, \"tags\": {\"_files\": []}}}, \".github\": {\"_files\": [\"PULL_REQUEST_TEMPLATE.md\"], \"ISSUE_TEMPLATE\": {\"_files\": [\"bug-report.yml\"]}, \"workflows\": {\"_files\": [\"build-docker-images-release.yml\", \"build_and_run_tests.yml\", \"build_docker_images.yml\", \"build_documentation.yml\", \"build_pr_documentation.yml\", \"integration_tests.yml\", \"nightly.yml\", \"quality.yml\", \"run_merge_tests.yml\", \"self_hosted_integration_tests.yml\", \"stale.yml\", \"test.yml\", \"test_imports.yml\", \"trufflehog.yml\", \"upload_pr_documentation.yml\"]}}, \"benchmarks\": {\"_files\": [\"README.md\"], \"big_model_inference\": {\"_files\": [\"big_model_inference.py\", \"measures_util.py\", \"README.md\"]}, \"fp8\": {\"_files\": [], \"ms_amp\": {\"_files\": [\"ddp.py\", \"distrib_deepspeed.py\", \"Dockerfile\", \"fp8_utils.py\", \"non_distributed.py\"]}, \"transformer_engine\": {\"_files\": [\"ddp.py\", \"distrib_deepspeed.py\", \"Dockerfile\", \"fp8_utils.py\", \"fsdp.py\", \"non_distributed.py\", \"README.md\"]}}}, \"docker\": {\"_files\": [\"README.md\"], \"accelerate-cpu\": {\"_files\": [\"Dockerfile\"]}, \"accelerate-gpu\": {\"_files\": [\"Dockerfile\"]}, \"accelerate-gpu-deepspeed\": {\"_files\": [\"Dockerfile\"]}}, \"docs\": {\"_files\": [\"Makefile\", \"README.md\"], \"source\": {\"_files\": [\"index.md\", \"quicktour.md\", \"_toctree.yml\"], \"basic_tutorials\": {\"_files\": [\"execution.md\", \"install.md\", \"launch.md\", \"migration.md\", \"notebook.md\", \"overview.md\", \"tpu.md\", \"troubleshooting.md\"]}, \"concept_guides\": {\"_files\": [\"big_model_inference.md\", \"deferring_execution.md\", \"fsdp_and_deepspeed.md\", \"gradient_synchronization.md\", \"internal_mechanism.md\", \"low_precision_training.md\", \"performance.md\", \"training_tpu.md\"]}, \"imgs\": {\"_files\": [\"accelerate_logo.png\", \"course_banner.png\", \"profile_export.png\"]}, \"package_reference\": {\"_files\": [\"accelerator.md\", \"big_modeling.md\", \"cli.md\", \"deepspeed.md\", \"fp8.md\", \"fsdp.md\", \"inference.md\", \"kwargs.md\", \"launchers.md\", \"logging.md\", \"megatron_lm.md\", \"state.md\", \"torch_wrappers.md\", \"tracking.md\", \"utilities.md\"]}, \"usage_guides\": {\"_files\": [\"big_modeling.md\", \"checkpoint.md\", \"ddp_comm_hook.md\", \"deepspeed.md\", \"deepspeed_multiple_model.md\", \"distributed_inference.md\", \"explore.md\", \"fsdp.md\", \"gradient_accumulation.md\", \"ipex.md\", \"local_sgd.md\", \"low_precision_training.md\", \"megatron_lm.md\", \"model_size_estimator.md\", \"mps.md\", \"profiler.md\", \"quantization.md\", \"sagemaker.md\", \"tracking.md\", \"training_zoo.md\"]}}}, \"examples\": {\"_files\": [\"complete_cv_example.py\", \"complete_nlp_example.py\", \"cv_example.py\", \"multigpu_remote_launcher.py\", \"nlp_example.py\", \"README.md\", \"requirements.txt\"], \"by_feature\": {\"_files\": [\"automatic_gradient_accumulation.py\", \"checkpointing.py\", \"cross_validation.py\", \"ddp_comm_hook.py\", \"deepspeed_with_config_support.py\", \"early_stopping.py\", \"fsdp_with_peak_mem_tracking.py\", \"gradient_accumulation.py\", \"local_sgd.py\", \"megatron_lm_gpt_pretraining.py\", \"memory.py\", \"multi_process_metrics.py\", \"profiler.py\", \"README.md\", \"schedule_free.py\", \"tracking.py\"]}, \"config_yaml_templates\": {\"_files\": [\"deepspeed.yaml\", \"fp8.yaml\", \"fsdp.yaml\", \"multi_gpu.yaml\", \"multi_node.yaml\", \"README.md\", \"run_me.py\", \"single_gpu.yaml\"]}, \"deepspeed_config_templates\": {\"_files\": [\"zero_stage1_config.json\", \"zero_stage2_config.json\", \"zero_stage2_offload_config.json\", \"zero_stage3_config.json\", \"zero_stage3_offload_config.json\"]}, \"inference\": {\"_files\": [], \"distributed\": {\"_files\": [\"distributed_image_generation.py\", \"phi2.py\", \"README.md\", \"stable_diffusion.py\"]}, \"pippy\": {\"_files\": [\"bert.py\", \"gpt2.py\", \"llama.py\", \"README.md\", \"requirements.txt\", \"t5.py\"]}}, \"slurm\": {\"_files\": [\"fsdp_config.yaml\", \"submit_multicpu.sh\", \"submit_multigpu.sh\", \"submit_multinode.sh\", \"submit_multinode_fsdp.sh\"]}}, \"manim_animations\": {\"_files\": [], \"big_model_inference\": {\"_files\": [\"stage_1.py\", \"stage_2.py\", \"stage_3.py\", \"stage_4.py\", \"stage_5.py\"]}, \"dataloaders\": {\"_files\": [\"stage_0.py\", \"stage_1.py\", \"stage_2.py\", \"stage_3.py\", \"stage_4.py\", \"stage_5.py\", \"stage_6.py\", \"stage_7.py\"]}}, \"src\": {\"_files\": [], \"accelerate\": {\"_files\": [\"accelerator.py\", \"big_modeling.py\", \"checkpointing.py\", \"data_loader.py\", \"hooks.py\", \"inference.py\", \"launchers.py\", \"local_sgd.py\", \"logging.py\", \"memory_utils.py\", \"optimizer.py\", \"scheduler.py\", \"state.py\", \"tracking.py\", \"__init__.py\"], \"commands\": {\"_files\": [\"accelerate_cli.py\", \"env.py\", \"estimate.py\", \"launch.py\", \"merge.py\", \"test.py\", \"tpu.py\", \"utils.py\", \"__init__.py\"], \"config\": {\"_files\": [\"cluster.py\", \"config.py\", \"config_args.py\", \"config_utils.py\", \"default.py\", \"sagemaker.py\", \"update.py\", \"__init__.py\"]}, \"menu\": {\"_files\": [\"cursor.py\", \"helpers.py\", \"input.py\", \"keymap.py\", \"selection_menu.py\", \"__init__.py\"]}}, \"test_utils\": {\"_files\": [\"examples.py\", \"testing.py\", \"training.py\", \"__init__.py\"], \"scripts\": {\"_files\": [\"test_cli.py\", \"test_ddp_comm_hook.py\", \"test_distributed_data_loop.py\", \"test_merge_weights.py\", \"test_notebook.py\", \"test_ops.py\", \"test_script.py\", \"test_sync.py\", \"__init__.py\"], \"external_deps\": {\"_files\": [\"test_checkpointing.py\", \"test_ds_multiple_model.py\", \"test_metrics.py\", \"test_peak_memory_usage.py\", \"test_performance.py\", \"test_pippy.py\", \"test_zero3_integration.py\", \"__init__.py\"]}}}, \"utils\": {\"_files\": [\"bnb.py\", \"constants.py\", \"dataclasses.py\", \"deepspeed.py\", \"environment.py\", \"fsdp_utils.py\", \"imports.py\", \"launch.py\", \"megatron_lm.py\", \"memory.py\", \"modeling.py\", \"offload.py\", \"operations.py\", \"other.py\", \"random.py\", \"rich.py\", \"torch_xla.py\", \"tqdm.py\", \"transformer_engine.py\", \"versions.py\", \"__init__.py\"]}}}, \"tests\": {\"_files\": [\"test_accelerator.py\", \"test_big_modeling.py\", \"test_cli.py\", \"test_cpu.py\", \"test_data_loader.py\", \"test_examples.py\", \"test_grad_sync.py\", \"test_hooks.py\", \"test_imports.py\", \"test_kwargs_handlers.py\", \"test_logging.py\", \"test_memory_utils.py\", \"test_metrics.py\", \"test_modeling_utils.py\", \"test_multigpu.py\", \"test_offload.py\", \"test_optimizer.py\", \"test_quantization.py\", \"test_sagemaker.py\", \"test_scheduler.py\", \"test_state_checkpointing.py\", \"test_tpu.py\", \"test_tracking.py\", \"test_utils.py\", \"xla_spawn.py\"], \"deepspeed\": {\"_files\": [\"ds_config_zero2.json\", \"ds_config_zero2_model_only.json\", \"ds_config_zero3.json\", \"ds_config_zero3_model_only.json\", \"test_deepspeed.py\", \"test_deepspeed_multiple_model.py\"]}, \"fsdp\": {\"_files\": [\"test_fsdp.py\"]}, \"test_configs\": {\"_files\": [\"0_11_0.yaml\", \"0_12_0.yaml\", \"0_28_0_mpi.yaml\", \"0_30_0_sagemaker.yaml\", \"0_34_0_fp8.yaml\", \"invalid_keys.yaml\", \"latest.yaml\", \"README.md\"]}, \"test_samples\": {\"_files\": [\"test_command_file.sh\"], \"MRPC\": {\"_files\": [\"dev.csv\", \"train.csv\"]}}}, \"utils\": {\"_files\": [\"log_reports.py\", \"stale.py\"]}}","readme_content":"<!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n    <br>\n    <img src=\"https:\/\/raw.githubusercontent.com\/huggingface\/accelerate\/main\/docs\/source\/imgs\/accelerate_logo.png\" width=\"400\"\/>\n    <br>\n<p>\n\n<p align=\"center\">\n    <!-- Uncomment when CircleCI is set up\n    <a href=\"https:\/\/circleci.com\/gh\/huggingface\/accelerate\"><img alt=\"Build\" src=\"https:\/\/img.shields.io\/circleci\/build\/github\/huggingface\/transformers\/master\"><\/a>\n    -->\n    <a href=\"https:\/\/github.com\/huggingface\/accelerate\/blob\/main\/LICENSE\"><img alt=\"License\" src=\"https:\/\/img.shields.io\/github\/license\/huggingface\/accelerate.svg?color=blue\"><\/a>\n    <a href=\"https:\/\/huggingface.co\/docs\/accelerate\/index.html\"><img alt=\"Documentation\" src=\"https:\/\/img.shields.io\/website\/http\/huggingface.co\/docs\/accelerate\/index.html.svg?down_color=red&down_message=offline&up_message=online\"><\/a>\n    <a href=\"https:\/\/github.com\/huggingface\/accelerate\/releases\"><img alt=\"GitHub release\" src=\"https:\/\/img.shields.io\/github\/release\/huggingface\/accelerate.svg\"><\/a>\n    <a href=\"https:\/\/github.com\/huggingface\/accelerate\/blob\/main\/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https:\/\/img.shields.io\/badge\/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"><\/a>\n<\/p>\n\n<h3 align=\"center\">\n<p>Run your *raw* PyTorch training script on any kind of device\n<\/h3>\n\n<h3 align=\"center\">\n    <a href=\"https:\/\/hf.co\/course\"><img src=\"https:\/\/raw.githubusercontent.com\/huggingface\/accelerate\/main\/docs\/source\/imgs\/course_banner.png\"><\/a>\n<\/h3>\n\n## Easy to integrate\n\n\ud83e\udd17 Accelerate was created for PyTorch users who like to write the training loop of PyTorch models but are reluctant to write and maintain the boilerplate code needed to use multi-GPUs\/TPU\/fp16.\n\n\ud83e\udd17 Accelerate abstracts exactly and only the boilerplate code related to multi-GPUs\/TPU\/fp16 and leaves the rest of your code unchanged.\n\nHere is an example:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n+ device = accelerator.device\n\n  model = torch.nn.Transformer().to(device)\n  optimizer = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optimizer, data = accelerator.prepare(model, optimizer, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n          source = source.to(device)\n          targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nAs you can see in this example, by adding 5-lines to any standard PyTorch training script you can now run on any kind of single or distributed node setting (single CPU, single GPU, multi-GPUs and TPUs) as well as with or without mixed precision (fp8, fp16, bf16).\n\nIn particular, the same code can then be run without modification on your local machine for debugging or your training environment.\n\n\ud83e\udd17 Accelerate even handles the device placement for you (which requires a few more changes to your code, but is safer in general), so you can even simplify your training loop further:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n- device = 'cpu'\n+ accelerator = Accelerator()\n\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\n  optimizer = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optimizer, data = accelerator.prepare(model, optimizer, data)\n\n  model.train()\n  for epoch in range(10):\n      for source, targets in data:\n-         source = source.to(device)\n-         targets = targets.to(device)\n\n          optimizer.zero_grad()\n\n          output = model(source)\n          loss = F.cross_entropy(output, targets)\n\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n```\n\nWant to learn more? Check out the [documentation](https:\/\/huggingface.co\/docs\/accelerate) or have a look at our [examples](https:\/\/github.com\/huggingface\/accelerate\/tree\/main\/examples).\n\n## Launching script\n\n\ud83e\udd17 Accelerate also provides an optional CLI tool that allows you to quickly configure and test your training environment before launching the scripts. No need to remember how to use `torch.distributed.run` or to write a specific launcher for TPU training!\nOn your machine(s) just run:\n\n```bash\naccelerate config\n```\n\nand answer the questions asked. This will generate a config file that will be used automatically to properly set the default options when doing\n\n```bash\naccelerate launch my_script.py --args_to_my_script\n``` \n\nFor instance, here is how you would run the GLUE example on the MRPC task (from the root of the repo):\n\n```bash\naccelerate launch examples\/nlp_example.py\n```\n\nThis CLI tool is **optional**, and you can still use `python my_script.py` or `python -m torchrun my_script.py` at your convenience.\n\nYou can also directly pass in the arguments you would to `torchrun` as arguments to `accelerate launch` if you wish to not run` accelerate config`.\n\nFor example, here is how to launch on two GPUs:\n\n```bash\naccelerate launch --multi_gpu --num_processes 2 examples\/nlp_example.py\n```\n\nTo learn more, check the CLI documentation available [here](https:\/\/huggingface.co\/docs\/accelerate\/package_reference\/cli).\n\nOr view the configuration zoo [here](https:\/\/github.com\/huggingface\/accelerate\/blob\/main\/examples\/config_yaml_templates\/)\n\n## Launching multi-CPU run using MPI\n\n\ud83e\udd17 Here is another way to launch multi-CPU run using MPI. You can learn how to install Open MPI on [this page](https:\/\/www.open-mpi.org\/faq\/?category=building#easy-build). You can use Intel MPI or MVAPICH as well.\nOnce you have MPI setup on your cluster, just run:\n```bash\naccelerate config\n```\nAnswer the questions that are asked, selecting to run using multi-CPU, and answer \"yes\" when asked if you want accelerate to launch mpirun.\nThen, use `accelerate launch` with your script like:\n```bash\naccelerate launch examples\/nlp_example.py\n```\nAlternatively, you can use mpirun directly, without using the CLI like:\n```bash\nmpirun -np 2 python examples\/nlp_example.py\n```\n\n## Launching training using DeepSpeed\n\n\ud83e\udd17 Accelerate supports training on single\/multiple GPUs using DeepSpeed. To use it, you don't need to change anything in your training code; you can set everything using just `accelerate config`. However, if you desire to tweak your DeepSpeed related args from your Python script, we provide you the `DeepSpeedPlugin`.\n\n```python\nfrom accelerate import Accelerator, DeepSpeedPlugin\n\n# deepspeed needs to know your gradient accumulation steps beforehand, so don't forget to pass it\n# Remember you still need to do gradient accumulation by yourself, just like you would have done without deepspeed\ndeepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=2)\naccelerator = Accelerator(mixed_precision='fp16', deepspeed_plugin=deepspeed_plugin)\n\n# How to save your \ud83e\udd17 Transformer?\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(save_dir, save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n```\n\nNote: DeepSpeed support is experimental for now. In case you get into some problem, please open an issue.\n\n## Launching your training from a notebook\n\n\ud83e\udd17 Accelerate also provides a `notebook_launcher` function you can use in a notebook to launch a distributed training. This is especially useful for Colab or Kaggle notebooks with a TPU backend. Just define your training loop in a `training_function` then in your last cell, add:\n\n```python\nfrom accelerate import notebook_launcher\n\nnotebook_launcher(training_function)\n```\n\nAn example can be found in [this notebook](https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/examples\/accelerate_examples\/simple_nlp_example.ipynb). [![Open In Colab](https:\/\/colab.research.google.com\/assets\/colab-badge.svg)](https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/main\/examples\/accelerate_examples\/simple_nlp_example.ipynb)\n\n## Why should I use \ud83e\udd17 Accelerate?\n\nYou should use \ud83e\udd17 Accelerate when you want to easily run your training scripts in a distributed environment without having to renounce full control over your training loop. This is not a high-level framework above PyTorch, just a thin wrapper so you don't have to learn a new library. In fact, the whole API of \ud83e\udd17 Accelerate is in one class, the `Accelerator` object.\n\n## Why shouldn't I use \ud83e\udd17 Accelerate?\n\nYou shouldn't use \ud83e\udd17 Accelerate if you don't want to write a training loop yourself. There are plenty of high-level libraries above PyTorch that will offer you that, \ud83e\udd17 Accelerate is not one of them.\n\n## Frameworks using \ud83e\udd17 Accelerate\n\nIf you like the simplicity of \ud83e\udd17 Accelerate but would prefer a higher-level abstraction around its capabilities, some frameworks and libraries that are built on top of \ud83e\udd17 Accelerate are listed below:\n\n* [Amphion](https:\/\/github.com\/open-mmlab\/Amphion) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.\n* [Animus](https:\/\/github.com\/Scitator\/animus) is a minimalistic framework to run machine learning experiments. Animus highlights common \"breakpoints\" in ML experiments and provides a unified interface for them within [IExperiment](https:\/\/github.com\/Scitator\/animus\/blob\/main\/animus\/core.py#L76).\n* [Catalyst](https:\/\/github.com\/catalyst-team\/catalyst#getting-started) is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop. Catalyst provides a [Runner](https:\/\/catalyst-team.github.io\/catalyst\/api\/core.html#runner) to connect all parts of the experiment: hardware backend, data transformations, model training, and inference logic.\n* [fastai](https:\/\/github.com\/fastai\/fastai#installing) is a PyTorch framework for Deep Learning that simplifies training fast and accurate neural nets using modern best practices. fastai provides a [Learner](https:\/\/docs.fast.ai\/learner.html#Learner) to handle the training, fine-tuning, and inference of deep learning algorithms.\n* [Finetuner](https:\/\/github.com\/jina-ai\/finetuner) is a service that enables models to create higher-quality embeddings for semantic search, visual similarity search, cross-modal text<->image search, recommendation systems, clustering, duplication detection, anomaly detection, or other uses.\n* [InvokeAI](https:\/\/github.com\/invoke-ai\/InvokeAI) is a creative engine for Stable Diffusion models, offering industry-leading WebUI, terminal usage support, and serves as the foundation for many commercial products.\n* [Kornia](https:\/\/kornia.readthedocs.io\/en\/latest\/get-started\/introduction.html) is a differentiable library that allows classical computer vision to be integrated into deep learning models. Kornia provides a [Trainer](https:\/\/kornia.readthedocs.io\/en\/latest\/x.html#kornia.x.Trainer) with the specific purpose to train and fine-tune the supported deep learning algorithms within the library.\n* [Open Assistant](https:\/\/projects.laion.ai\/Open-Assistant\/) is a chat-based assistant that understands tasks, can interact with their party systems, and retrieve information dynamically to do so. \n* [pytorch-accelerated](https:\/\/github.com\/Chris-hughes10\/pytorch-accelerated) is a lightweight training library, with a streamlined feature set centered around a general-purpose [Trainer](https:\/\/pytorch-accelerated.readthedocs.io\/en\/latest\/trainer.html), that places a huge emphasis on simplicity and transparency; enabling users to understand exactly what is going on under the hood, but without having to write and maintain the boilerplate themselves!\n* [Stable Diffusion web UI](https:\/\/github.com\/AUTOMATIC1111\/stable-diffusion-webui) is an open-source browser-based easy-to-use interface based on the Gradio library for Stable Diffusion.\n* [torchkeras](https:\/\/github.com\/lyhue1991\/torchkeras) is a simple tool for training pytorch model just in a keras style, a dynamic and beautiful plot is provided in notebook to monitor your loss or metric.\n* [transformers](https:\/\/github.com\/huggingface\/transformers) as a tool for helping train state-of-the-art machine learning models in PyTorch, Tensorflow, and JAX. (Accelerate is the backend for the PyTorch side).\n\n\n## Installation\n\nThis repository is tested on Python 3.8+ and PyTorch 1.10.0+\n\nYou should install \ud83e\udd17 Accelerate in a [virtual environment](https:\/\/docs.python.org\/3\/library\/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https:\/\/packaging.python.org\/guides\/installing-using-pip-and-virtual-environments\/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install PyTorch: refer to the [official installation page](https:\/\/pytorch.org\/get-started\/locally\/#start-locally) regarding the specific install command for your platform. Then \ud83e\udd17 Accelerate can be installed using pip as follows:\n\n```bash\npip install accelerate\n```\n\n## Supported integrations\n\n- CPU only\n- multi-CPU on one node (machine)\n- multi-CPU on several nodes (machines)\n- single GPU\n- multi-GPU on one node (machine)\n- multi-GPU on several nodes (machines)\n- TPU\n- FP16\/BFloat16 mixed precision\n- FP8 mixed precision with [Transformer Engine](https:\/\/github.com\/NVIDIA\/TransformerEngine) or [MS-AMP](https:\/\/github.com\/Azure\/MS-AMP\/)\n- DeepSpeed support (Experimental)\n- PyTorch Fully Sharded Data Parallel (FSDP) support (Experimental)\n- Megatron-LM support (Experimental)\n\n## Citing \ud83e\udd17 Accelerate\n\nIf you use \ud83e\udd17 Accelerate in your publication, please cite it by using the following BibTeX entry.\n\n```bibtex\n@Misc{accelerate,\n  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},\n  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},\n  howpublished = {\\url{https:\/\/github.com\/huggingface\/accelerate}},\n  year =         {2022}\n}\n```\n"}
{"repo_id":"action-check-commits","file_structure":"{\".\": {\"_files\": [\".gitignore\", \".prettierrc.js\", \"action.yml\", \"babel.config.js\", \"CONTRIBUTING.md\", \"index.js\", \"jest.config.js\", \"package.json\", \"README.md\", \"tsconfig.json\"]}, \".git\": {\"_files\": [\"config\", \"description\", \"HEAD\", \"index\", \"packed-refs\"], \"hooks\": {\"_files\": [\"applypatch-msg.sample\", \"commit-msg.sample\", \"fsmonitor-watchman.sample\", \"post-update.sample\", \"pre-applypatch.sample\", \"pre-commit.sample\", \"pre-merge-commit.sample\", \"pre-push.sample\", \"pre-rebase.sample\", \"pre-receive.sample\", \"prepare-commit-msg.sample\", \"push-to-checkout.sample\", \"sendemail-validate.sample\", \"update.sample\"]}, \"info\": {\"_files\": [\"exclude\"]}, \"logs\": {\"_files\": [\"HEAD\"], \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"main\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}}}, \"objects\": {\"_files\": [], \"info\": {\"_files\": []}, \"pack\": {\"_files\": [\"pack-7a7722ba0e135eacb8078f20741c9adec62cb477.idx\", \"pack-7a7722ba0e135eacb8078f20741c9adec62cb477.pack\", \"pack-7a7722ba0e135eacb8078f20741c9adec62cb477.rev\"]}}, \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"main\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}, \"tags\": {\"_files\": []}}}, \"dist\": {\"_files\": [], \"main\": {\"_files\": [\"index.js\"]}}, \"src\": {\"_files\": [\"extractCommits.ts\", \"isValidCommitMessage.ts\", \"main.ts\"], \"__tests__\": {\"_files\": [\"isValidCommitMessage.test.ts\"]}}}","readme_content":"# Check Commit Messages GitHub Action\n\nA simple GitHub action that checks the list of commits in a pull-request:\n\n- the number of commits shall not be higher than `max-commits` (defaults to 10),\n- each commit message must at least contain `min-words` (defaults to 3),\n- each commit message must not contain any `forbidden-words` (like `fixup`).\n\nHeavily inspired by [webiny\/action-conventional-commits](https:\/\/github.com\/webiny\/action-conventional-commits).\n\n\n### Usage\nLatest version: `v0.0.1`\n\n```yml\nname: Check commit messages\n\non:\n  pull_request:\n    branches: [ master ]\n\njobs:\n  build:\n    name: Check Commits\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions\/checkout@v3\n\n      - uses: huggingface\/check-commits@v0.0.1\n        with:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Optional, for private repositories.\n          max-commits: \"15\" # Optional, defaults to 10.\n          min-words: \"5\" # Optional, defaults to 3.\n          forbidden-words: [\"fixup', \"wip\"] # Optional, defaults to [\"fixup\"].\n```\n"}
{"repo_id":"adversarialnlp","file_structure":"{\".\": {\"_files\": [\".gitignore\", \".pylintrc\", \"pytest.ini\", \"README.md\", \"readthedocs.yml\", \"requirements.txt\", \"setup.cfg\", \"setup.py\"]}, \".git\": {\"_files\": [\"config\", \"description\", \"HEAD\", \"index\", \"packed-refs\"], \"hooks\": {\"_files\": [\"applypatch-msg.sample\", \"commit-msg.sample\", \"fsmonitor-watchman.sample\", \"post-update.sample\", \"pre-applypatch.sample\", \"pre-commit.sample\", \"pre-merge-commit.sample\", \"pre-push.sample\", \"pre-rebase.sample\", \"pre-receive.sample\", \"prepare-commit-msg.sample\", \"push-to-checkout.sample\", \"sendemail-validate.sample\", \"update.sample\"]}, \"info\": {\"_files\": [\"exclude\"]}, \"logs\": {\"_files\": [\"HEAD\"], \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"master\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}}}, \"objects\": {\"_files\": [], \"info\": {\"_files\": []}, \"pack\": {\"_files\": [\"pack-6878d0af217b08235082f482e52ab15480c21d0f.idx\", \"pack-6878d0af217b08235082f482e52ab15480c21d0f.pack\", \"pack-6878d0af217b08235082f482e52ab15480c21d0f.rev\"]}}, \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"master\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}, \"tags\": {\"_files\": []}}}, \"adversarialnlp\": {\"_files\": [\"run.py\", \"version.py\", \"__init__.py\"], \"commands\": {\"_files\": [\"test_install.py\", \"__init__.py\"]}, \"common\": {\"_files\": [\"file_utils.py\", \"__init__.py\"]}, \"generators\": {\"_files\": [\"generator.py\", \"__init__.py\"], \"addsent\": {\"_files\": [\"addsent_generator.py\", \"corenlp.py\", \"squad_reader.py\", \"utils.py\", \"__init__.py\"], \"rules\": {\"_files\": [\"alteration_rules.py\", \"answer_rules.py\", \"conversion_rules.py\", \"__init__.py\"]}}, \"swag\": {\"_files\": [\"activitynet_captions_reader.py\", \"openai_transformer_model.py\", \"simple_bilm.py\", \"swag_generator.py\", \"utils.py\", \"__init__.py\"]}}, \"pruners\": {\"_files\": [\"pruner.py\", \"__init__.py\"]}, \"tests\": {\"_files\": [\"__init__.py\"], \"dataset_readers\": {\"_files\": [\"activitynet_captions_test.py\", \"__init__.py\"]}, \"fixtures\": {\"_files\": [\"activitynet_captions.json\", \"squad.json\"]}, \"generators\": {\"_files\": [\"addsent_generator_test.py\", \"swag_generator_test.py\", \"__init__.py\"]}}}, \"bin\": {\"_files\": [\"adversarialnlp\"]}, \"docs\": {\"_files\": [\"common.rst\", \"conf.py\", \"generators.rst\", \"index.rst\", \"make.bat\", \"Makefile\", \"readme.rst\", \"readthedoc_requirements.txt\"]}, \"tutorials\": {\"_files\": [\"usage.py\"]}}","readme_content":"# AdversarialNLP - WIP\n\nAdversarialNLP is a generic library for crafting and using Adversarial NLP examples.\n\nWork in Progress\n\n## Installation\n\nAdversarialNLP requires Python 3.6.1 or later. The preferred way to install AdversarialNLP is via `pip`. Just run `pip install adversarialnlp` in your Python environment and you're good to go!\n"}
{"repo_id":"AI-Canvas","file_structure":"{\".\": {\"_files\": [\".gitignore\", \"Handtracking.py\", \"HandTrackingModule.py\", \"README.md\", \"requirements.txt\", \"VirtualPainter.py\"]}, \".git\": {\"_files\": [\"config\", \"description\", \"HEAD\", \"index\", \"packed-refs\"], \"hooks\": {\"_files\": [\"applypatch-msg.sample\", \"commit-msg.sample\", \"fsmonitor-watchman.sample\", \"post-update.sample\", \"pre-applypatch.sample\", \"pre-commit.sample\", \"pre-merge-commit.sample\", \"pre-push.sample\", \"pre-rebase.sample\", \"pre-receive.sample\", \"prepare-commit-msg.sample\", \"push-to-checkout.sample\", \"sendemail-validate.sample\", \"update.sample\"]}, \"info\": {\"_files\": [\"exclude\"]}, \"logs\": {\"_files\": [\"HEAD\"], \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"main\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}}}, \"objects\": {\"_files\": [], \"info\": {\"_files\": []}, \"pack\": {\"_files\": [\"pack-ba8e1c10fd4356380e9f6edec75c5b00e2f3ed63.idx\", \"pack-ba8e1c10fd4356380e9f6edec75c5b00e2f3ed63.pack\", \"pack-ba8e1c10fd4356380e9f6edec75c5b00e2f3ed63.rev\"]}}, \"refs\": {\"_files\": [], \"heads\": {\"_files\": [\"main\"]}, \"remotes\": {\"_files\": [], \"origin\": {\"_files\": [\"HEAD\"]}}, \"tags\": {\"_files\": []}}}, \"Header\": {\"_files\": [\"1.jpg\", \"2.jpg\", \"3.jpg\", \"4.jpg\"]}, \"__pycache__\": {\"_files\": [\"Handtracking.cpython-312.pyc\", \"HandTrackingModule.cpython-312.pyc\"]}}","readme_content":"# Virtual AI Canvas with OpenCV and MediaPipe\n\n## Table of Content\n- [Overview](#overview)\n- [Demo](#demo)\n- [Motivation](#motivation)\n- [Technical Aspect](#technical-aspect)\n- [Installation And Run](#installation-and-run)\n- [Directory Tree](#directory-tree)\n- [To Do](#to-do)\n- [Bug \/ Feature Request](#bug---feature-request)\n- [Technologies Used](#technologies-used)\n- [Credits](#credits)\n  \n## Overview\nThis project is a hand tracking application built using OpenCV and MediaPipe. The application can detect hands in real-time from a webcam feed, track the position of hand landmarks, and provide functionality for drawing and erasing on the screen using hand gestures.\n\n## Demo\nHere's a brief demonstration of the hand tracking application in action:\n\n\n![Untitled video - Made with Clipchamp](https:\/\/github.com\/hamza-amin-4365\/AI-Canvas\/assets\/125562989\/354fce6a-d52e-41cd-a3bb-54208792fdab)\n\n\n## Motivation\nThe motivation behind this project was to explore the capabilities of computer vision and hand tracking technologies, and to create an interactive application that demonstrates these capabilities in a fun and engaging way.\n\n## Technical Aspect\nThe application uses the following technologies and libraries:\n\n- **Opencv**: An open-source computer vision and machine learning software library.\n- **MediaPipe**: A cross-platform framework developed by Google for building multimodal applied machine learning pipelines.\n- **Numpy**: A library for the Python programming language, adding support for large, multi-dimensional arrays and matrices.\n\nThe application follows these steps:\n\n1. Capture video from a webcam.\n2. Use MediaPipe to detect and track hand landmarks in the video frames.\n3. Implement different modes for drawing and erasing on the screen based on hand gestures.\n4. Overlay the drawing on the video feed and display it to the user.\n\n## Installation And Run\n1. Clone the repository or download the source code.\n2. Install the required packages by running the following command:\n\n```bash\npip install -r requirements.txt\n```\n3. Run the application with the following command:\n```bash\npython VirtualPainter.py\n```\nDirectory Tree\n```\n\u2502   app.py\n\u2502   HandTrackingModule.py\n\u2502   README.md\n\u2502   requirements.txt\n\u2514\u2500\u2500\u2500Header\n        # Images for header will be stored here\n```\n## To Do\n\n* Improve hand tracking accuracy and responsiveness.\n* Add support for more gestures and interactions.\n* Optimize performance for lower-end hardware.\n\n## Bug \/ Feature Request\nIf you find a bug or have a feature request, please open an issue [here](https:\/\/github.com\/hamza-amin-4365\/AI-Canvas\/issues\/new).\n\n## Technologies Used\n<img target=\"_blank\" src=\"https:\/\/opencv.org\/wp-content\/uploads\/2020\/07\/OpenCV_logo_black.png\" width=200>\n<img target=\"_blank\" src=\"https:\/\/mediapipe.dev\/images\/logo_horizontal_color.png\" width=200>\n<img target=\"_blank\" src=\"https:\/\/numpy.org\/images\/logo.svg\" width=200>\n\n## Credits\n* MediaPipe\n* OpenCV\n* NumPy\n"}
